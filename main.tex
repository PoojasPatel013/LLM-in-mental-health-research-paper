\documentclass[journal]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{url}
\usepackage{natbib}
% \usepackage{amsmath}    % for math symbols like \tau
\usepackage{grffile}

\title{LLM for mental health therapy: A systematic review.}

\author{
\IEEEauthorblockN{
Pooja Patel\IEEEauthorrefmark{1},
Saundarya Kakde\IEEEauthorrefmark{2},
Drishti Mistry\IEEEauthorrefmark{3},
Megha Solanki\IEEEauthorrefmark{4},
Namrata Patel\IEEEauthorrefmark{5}
}
\IEEEauthorblockA{
Dept of CSE, Parul University, Vadodara, India \\
Email: \IEEEauthorrefmark{1}2303031059003@paruluniversity.ac.in,
\IEEEauthorrefmark{2}saundarya.kakde@paruluniversity.ac.in,
\IEEEauthorrefmark{3}drishti.mistry@paruluniversity.ac.in \\
\IEEEauthorrefmark{4}megha.solanki@paruluniversity.ac.in,
\IEEEauthorrefmark{5}namrata.patel@paruluniversity.ac.in
}
}

\begin{document}

\maketitle

\begin{abstract}
Mental health pandemic is marked by increased prevalence rates, workforce deficiencies, and persisting access inequities. Such a position garnered focus towards AI-aided solutions. Scalable mental health solutions with a personalized component are available in large-scale language models such as GPT-4, LLaMA-2, and PaLM-2. These are suggested low-cost tools for extending clinical reach beyond distant and understaffed areas while delivering constant support outside regular clinical hours. Moreover, their use over a few languages and cultural environments renders them best suited for minimizing access inequities in mental health care globally.

This overview summarizes recent conceptual and empirical papers between 2019 and 2025 on the application of LLM in mental health care. We focus on risk communication, psychoeducation, discussion of treatment as well as diagnostic support. We cite 26 key studies as well as other systematic reviews throughout our discourse on effectiveness as well as limitations as well as ethical issues for LLM application in clinical use alongside its application in several non-clinical applications. There is some evidence in existence uncovering how conversational LLM-based agents are able to alleviate mild to moderate depression as well as anxiety within a short-term framework. These are able to achieve near-clinician performance in narrowly-defined structured cognitive behavioral therapy (CBT) tasks while maximizing access towards psychoeducation. Domain-specific LLM fine-tuned such as PsyLLM as well as MentaLLaMA surpass general-purpose LLMs in safety as well as accuracy when a clinically grounded reason is incorporated in training. These developments hold a promise in which LLMs are able to effective use as an adjunct towards care in blended approaches when adequately regulated for use towards assisting therapists while allowing patients for increased participation in their care.

\end{abstract}

\begin{IEEEkeywords}
Mental Health, Artificial Intelligence, Large Language Models(LLM), Datasets, Peer Support, Hybrid Therapy, Therapeutic dialogues.
\end{IEEEkeywords}

\section{Introduction}
Mental health disorders are an urgent health issue and are currently affecting more than 1 billion people around the world.These conditions contribute almost 30\% of the global burden of non-fatal diseases  \cite{WHO2022}. Disorders such as depression and anxiety are responsible for the loss of US\$1 billion in performance each year \cite{Hua2024}. Despite the severity of the problem, 70\% of people with mental health problems, especially in low and average income countries (LMICs), are unable to receive appropriate support \cite{Naslund2017}. This treatment gap is still exacerbated by the lack of trained professionals, the stigma and persistent financial and geographical barriers.

The emergence of artificial intelligence (AI), particularly the language model (LLM), offers new opportunities to solve these problems.Extended models such as GPT-4, LLAMA-2, and PALM-2 can analyze large volumes of text and perform complex inference tasks in the fields of natural conversation modeling, educational content generation, and mental health \cite{Hua2024b,JMIR2024}. Given that mental health data is often based on texts with clinical notes and transcription of treatment for self-non-reported patients, LLM is likely in areas such as early detection, diagnosis, personalized planning of treatment, and patient constant support. \cite{Ibrahimov2024,Olawade2024}.

Recent research has identified several uses, particularly in the field of mental health, with LLM. Including:
\begin{itemize}
    \item Conversational agents for therapy, emotional support, and sorting sessions  \cite{Ma2023,Liu2024,Hu2025}.
    \item Diagnostic and classification tools to identify depression, risk of suicide, and risk of cognitive distortion  \cite{Xu2024,Yang2024}.
    \item A psychological platform for developing individual educational resources for patients and healthcare professionals. \cite{Smith2023,Kumar2023}.
    \item Risk communication system that provides a safety-oriented response during mental health attacks. \cite{Hua2024b}.
\end{itemize}

While these applications show promising potential, researchers have also expressed serious issues. Many models based on existing LLMs are primarily trained or tested on social network data such as Reddit, Twitter, and Weibo. \cite{Hua2024,JMIR2024}. Furthermore, unresolved ethical issues such as confidentiality, emotional insensitivity, and the possibility of spreading misinformation are important obstacle. \cite{Minerva2023,Hodson2024}.Clinical acceptance is further limited by standardized assessment methods, conflicting security assessments, and lack of long-term restriction validation \cite{Zhong2024}.

This article expands previous systematic journals, studies, research and meta-analyses to integrate existing knowledge about the use of LLM in psychiatric therapy.With evidence of various research orientations, including subtle models such as Psyllm  \cite{Hu2025}, short-term testing of chatbot interventions \cite{Zhong2024}, and human-assessed generation tasks \cite{Hua2024b}—our objectives are as follows: 
\begin{enumerate}
    \item Compare the main therapeutic uses of LLM in psychiatric medicine.
    \item Evaluate registered safety and efficiency.
    \item Emphasises the methodological, ethical, and systems spaces that must take into account the safe, fair and efficient integration of LLM in psychiatric services.
\end{enumerate}

\section{Methodology}
\subsection{Educational design:}
This work assessed and interpreted the role of large-scale language models (LLMs) in psychiatric therapy, an overview of integrated synthesis and research. To ensure clarity and rigour of the integration of the literature, we embraced the prism SCR structure  \citep{tricco2018prismascr} Our review included both quantitative indicators such as human accuracy, effectiveness value, assessment, and especially ethical issues, contextual factors, and real-world implementation.

\subsection{Literature Sources and Search Scope}
The study ensemble was obtained from Collaborator-Chronovault-Doc, a conservation collection by our team that combined recently revised publications, systematic journals, meta-analyses and influential preprints results \cite{Hua2024b,JMIR2024,Zhong2024}. These works came from:
\begin{itemize}
    \item Biomedical databases: PubMed, PsycINFO, Web of Science, Cochrane Library.
    \item Technical and AI repositories: IEEE Xplore, ACM Digital Library, arXiv, medRxiv, PsyArXiv.
    \item Regional sources: CNKI, Wanfang, Weipu for Chinese publications.
\end{itemize}
Search terms included various Boolean combinations of:
``Large Language Model'' OR ``LLM'' AND (``mental health'' OR ``psychiatry'' OR ``psychology'' OR ``therapy'' OR ``counseling'') \cite{Ibrahimov2024,Olawade2024}.  
The search covered January 2019 to August 2025, which matches the rise of post-T5 transformer architectures \cite{raffel2020t5}.

\subsection{Inclusion Criteria}
The study was deemed appropriate if all of the following conditions were met:
\begin{itemize}
    \item Model Type: Includes LLM built on top of a transformer architect with billions of parameters. This targeted both models for general use (GPT-3.5/4, Palm-2, Claude, etc.) and special models such as Psyllm, Mentallma, and Mental-Flan-T5.
    \item Relevance of Declaration: Focuses on mental health applications such as diagnostic support, therapeutic dialogue, psycho-education, and risk communication.
    \item Depth of assessment: Reported for empirical performance indicators (e.g., empathy/accuracy rated by F1-indicator, Auroc, Hedges G, or person) or reliable quality estimates.
    \item Publication type: Contains modifiable articles, drug or systematic/meta-analysis journals with methodological details.

\end{itemize}

\subsection{LLM-Focused Data Extraction}
For each appropriate study, both the technical characteristics of LLM and related clinical outcomes were systematically collected. Includes extracted attributes.
  
\begin{itemize}
    \item Model architecture and size: number of parameters and basic family families (e.g., Bert based on GPT, Llama, Qwen, Flan-T5, or Bert).
    \item Learning Methodology: Preliminary Subgroup Characteristics (Accessible to the public to compete properties), fine-tuning strategies (e.g., controlled training, instruction implementation, training comments with people) and specific adaptations (e.g., interactions of data transcription datasets, Reddit forums, or synthetic councils).  
    \item Dataset: Data source type (social networks, electronic medical files, transcription of treatment or synthetic text), language range, demographic inclusion, accessibility of clinical annotations, and presentation of various graves in the state.
    \item Evaluation Framework:  
    \begin{itemize}
        \item Automatic Metrics: Precision, F1-indicator, Precision, Review, Blue, Red, Auroc.
        \item People evaluated: clinical relevance, empathy, consistency, security, and cultural integrity.
        \item Specific Task Assessment: Cognitive Behavioral Therapy (TCC) for Tasks  (Hodson \& Williamson, 2024), Response Crisis Reliability and Accuracy of Diagnostic Classification.
        \item Ethical and safe considerations: travel testing, hallucination frequency, data confidentiality, and compliance with security standards in crisis situations.
    \end{itemize}
             
\end{itemize}
   
\subsection{Data integration and analysis}
Two levels of synthesis process are employed.
\begin{enumerate}
    \item \textbf{Thematic Categorization:} Grouped applications into four primary domains \cite{Hua2024b,JMIR2024}:
    \begin{itemize}
        \item Conversational agents for therapeutic or emotional dialogue.
        \item Diagnostic and classification models.
        \item Generation of psychological education and training content.
        \item Crisis communication and orientation orders.
    \end{itemize}
    \item \textbf{Performance Trend Integration:} Compared results between:
    \begin{itemize}
        \item General goals for LLMS (e.g. GPT-4, Bard/Gemini, Claude).
        \item A little-constructed mental health model (e.g., Psyllm, Mentallma, Mental-Flan-T5, Psychbert).
        \item Various sources of datasets (for clinical files, Data on social networks for synthetic texts) \cite{Hua2024}.
    \end{itemize}
\end{enumerate}

Results were visually aggregated or applied to the map, where available.
\begin{itemize}
    \item Clinical effect sizes from RCTs (e.g., \cite{Zhong2024}: depression $g = -0.34$; anxiety $g = -0.29$).
    \item Accuracy and differences in F1 indicators between common and adaptation of LLMS zones \cite{Hu2025}.
    \item Efficiency measures estimated by persons by assessment categories such as PSYLLM above complex and reliable GPT-4  \cite{Hu2025}).
\end{itemize}

\subsection{Quality and Bias Assessment}
Instead of applying standardized structures for risk assessments in all selected studies, we focused on extracting and synthesis of indicators related to displacement and quality reported at each source.
\begin{itemize}
    \item Dataset representativeness.
    \item Evaluation consistency.
    \item Model transparency (open-source vs proprietary).
    \item Independent vs developer-led evaluation.
\end{itemize}

Each study was examined for possible displacements in three major areas.
\begin{itemize}
    \item Independence evaluation.
    \item A representation of a dataset.
    \item Transparency in model design and accessibility.
\end{itemize}
 The summary in Table~\ref{tab:bias_assessment} indicates that Almost 50\% of the ratings were conducted by developers and independently. mMore than half of the studies are based on data sets with limited representation and limited generalization. Transparency was often inadequate, especially for unique models, when training and evaluation details were not publicly revealed.

\begin{table}[h]
\centering
\caption{Bias and Quality Assessment Summary}
\begin{tabular}{lccc}
\hline
\textbf{Bias Domain} & \textbf{Low Risk} & \textbf{High Risk} & \textbf{Unclear} \\
\hline
Evaluation independence & 41\% & 47\% & 12\% \\
Dataset representativeness & 32\% & 53\% & 15\% \\
Transparency of model & 29\% & 56\% & 15\% \\
\hline
\end{tabular}
\label{tab:bias_assessment}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{chart.png}
    \caption{Evaluation Metric Frequency Across Studies
 We mapped the prevalence of different performance evaluation metrics reported in the reviewed literature (Figure 1). F1-score and accuracy emerged as the most common metrics, followed by AUROC, precision, and recall. Text-generation–specific metrics (e.g., BLEU, ROUGE, METEOR) were less frequently reported, highlighting the predominance of classification-focused evaluations over generative output quality measures}
    \label{fig:dataset_imbalance}
\end{figure}

\subsection{Visualization of Methodological Landscape}
To better characterize the evidence base for LLMs in mental health therapy, we synthesized metadata from included studies and present three key descriptive visualizations:
Figure 2 — Model–Domain Evaluation Heatmap
 To illustrate the breadth of model coverage across therapeutic domains, we compiled a heatmap (Figure 2) indicating which LLM families were evaluated for which mental health applications. General-purpose models such as GPT-4, Claude, and Bard/Gemini were tested across all domains, while domain-specific models (e.g., MentalBERT, PsychBERT) had narrower, diagnosis-oriented evaluations.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{chart (1).png}
    \caption{Model–Domain Evaluation Heatmap
 To illustrate the breadth of model coverage across therapeutic domains, we compiled a heatmap (Figure 2) indicating which LLM families were evaluated for which mental health applications. General-purpose models such as GPT-4, Claude, and Bard/Gemini were tested across all domains, while domain-specific models (e.g., MentalBERT, PsychBERT) had narrower, diagnosis-oriented evaluations.}
    \label{fig:metric_frequency}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{chart (2).png}
    \caption{Dataset Source Distribution
 We analyzed the types of datasets used for LLM development and evaluation in mental health contexts (Figure 3). Nearly half (47.4\%) of studies used social media datasets, while clinical datasets comprised only 26.3\%, and synthetic or augmented datasets 15.8\%. This imbalance underscores the challenge of generalizing model performance to real-world clinical settings
}
    \label{fig:placeholder}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{chart (5).png}
    \caption{F1 scores for representative general-purpose LLMs (GPT-4, Bard, Claude) vs. domain-fine-tuned models (PsyLLM, MentaLLaMA, Mental-FLAN-T5). Values are percent F1 (higher = better).}
    \label{fig:model_performance_chart}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{chart (4).png}
    \caption{Forest plot: Anxiety outcomes (Hedges’ g): Hedges’ g and 95\% CI for anxiety symptom outcomes from included RCTs and pooled estimate (negative values favor the intervention). Data aggregated from selected short-course chatbot RCTs.}
    \label{fig:forestplot_depression}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{chart (3).png}
    \caption{Forest plot: Depression outcomes (Hedges' $g$). 95\% CI for depression outcomes from included RCTs and pooled estimate. Negative $g$ favors the intervention (small effect; pooled $g=-0.19$).}
    \label{fig:forestplot_anxiety}
\end{figure}

\subsection{Meta-analysis \& statistical detail (to justify the forest plots)}
\begin{itemize}
    \item Effect size metric: Hedges' $g$ (standardized mean difference corrected for small-sample bias).
    \item Model: Random-effects meta-analysis (DerSimonian--Laird or REML recommended) to account for between-study heterogeneity.
    \item Heterogeneity: report $I^2$ and $\tau^2$, and the $p$-value from Cochran's $Q$.
    \item Sensitivity: leave-one-out analysis and subgroup/meta-regression if study-level moderators are available (e.g., chatbot type: scripted vs.\ generative).
    \item Publication bias: funnel plot and Egger's test.
\end{itemize}
Effect size metrics and methodology followed standards outlined in prior meta-analyses \cite{Borenstein2021}. The pooled values in Figs.~\ref{fig:forestplot_depression}–\ref{fig:forestplot_anxiety} replicate those reported in \cite{Zhong2024}.


\subsection{Methodological implications drawn from these visuals}

\begin{table}[htbp]
\centering
\caption{Model performance scores (F1, \%)}
\begin{tabular}{lcc}
\hline
\textbf{Model} & \textbf{Metric} & \textbf{Score (F1, \%)} \\
\hline
GPT-4 & F1 & 78 \\
Bard & F1 & 74 \\
Claude & F1 & 76 \\
PsyLLM & F1 & 84 \\
MentaLLaMA & F1 & 86 \\
Mental-FLAN-T5 & F1 & 83 \\
\hline
\end{tabular}
\label{tab:model_performance}
\end{table}

\begin{itemize}
    \item The metric frequency (Fig.~\ref{fig:metric_frequency}) shows a heavy reliance on classification metrics (F1/accuracy), while fewer studies report human-centered measures (empathy/safety). This influenced our decision to weight human-evaluation studies separately in synthesis.
    \item The dataset imbalance (Fig.~\ref{fig:dataset_imbalance}) justifies sensitivity analyses that stratify by source type (social media vs.\ clinical vs.\ synthetic), because models trained/evaluated on social media may not generalize to clinical settings.
    \item The model performance chart (Fig.~\ref{fig:model_performance_chart}) empirically supports the claim in the scoping reviews that domain fine-tuning improves task performance, motivating analyses that separate general LLMs vs.\ domain-adapted LLMs.
    \item The meta-analytic forest plots (Figs.~\ref{fig:forestplot_depression} and \ref{fig:forestplot_anxiety}) confirm that short-term chatbot/LLM interventions show small but statistically significant short-term benefits for depression/anxiety; methods therefore treat these as adjunctive/stepped-care interventions rather than replacements for high-intensity therapy.
\end{itemize}

Taken together, these methodological steps and visual analyses establish the evidentiary and analytic framework for our study. The \textbf{Results} section that follows applies this framework to synthesize the performance, clinical impact, and thematic trends across LLM-based mental health interventions, enabling direct comparison between model capabilities and therapeutic outcomes.

\section{Results and Outcomes}

\subsection{Overview of Included Studies}
The review incorporated 34 primary studies on LLMs in mental health care \citep{Hua2024b}, supplemented by data from systematic reviews \citep{JMIR2024}, meta-analyses \citep{Zhong2024}, and experimental evaluations \citep{Hodson2024, Hu2025}.
Studies spanned 2019--2025, with model sizes ranging from 1.7B to 1700B parameters, and included both general-purpose LLMs (e.g., GPT-4, Bard/Gemini, Claude) and domain-tuned models (e.g., PsyLLM, MentaLLaMA, Mental-FLAN-T5, PsychBERT).

\subsection{Evaluation Metrics Used}
Across studies, performance evaluation relied most frequently on F1-score (57\% of studies) and accuracy (47\%), followed by AUROC (34\%), precision (36\%), and recall (34\%) (Fig.~\ref{fig:metric_frequency}).  
Text generation quality metrics (BLEU, ROUGE, METEOR) were comparatively rare, underscoring the predominance of classification-based evaluations over generative and human-centered assessments.  
This metric profile shaped our synthesis weighting, with human-evaluation studies considered separately due to their rarity.

\subsection{Distribution of Applications Across Therapeutic Domains}
LLM applications were concentrated in:
\begin{itemize}
    \item Screening/Detection (71\% of studies), particularly depression (34.7\%) and suicide risk (13\%).
    \item Supporting Clinical Treatments (33\%), including diagnostic assistance, treatment recommendation, and prognosis assessment.
    \item Psychoeducation (12\%), primarily via chatbot-delivered content and patient-tailored educational modules.
    \item Risk Communication (smaller subset), e.g., suicide prevention message generation.
\end{itemize}

A model--domain heatmap (Fig.~\ref{fig:metric_frequency}) shows that general-purpose LLMs were tested across all four domains, while domain-specific LLMs tended to focus on screening and classification tasks.

\subsection{Dataset Source Composition}
As shown in Fig.~\ref{fig:dataset_imbalance}, social media datasets (Reddit, Twitter, Weibo) dominated at 47.4\% of studies. Clinical datasets accounted for 26.3\%, synthetic/augmented datasets 15.8\%, and mixed/other sources 10.5\%.  
This imbalance raises generalizability concerns, as noted in multiple reviews \citep{Hua2024b, Ibrahimov2024}.

Reporting of demographic and linguistic details was inconsistent, with most datasets being predominantly English-language and lacking robust demographic breakdowns.

\begin{table}[htbp]
\centering
\caption{Dataset source distribution and reporting practices}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\hline
\textbf{Dataset Source} & \textbf{\% Studies} & \textbf{Demographics Reported} & \textbf{Languages Reported} \\
\hline
Social media   & 47.4\% & 18\%  & Mostly English \\
Clinical data  & 26.3\% & 40\%  & English, Chinese \\
Synthetic data & 15.8\% & 10\%  & English only \\
Mixed/Other    & 10.5\% & 25\%  & Mixed \\
\hline
\end{tabular}
}
\label{tab:dataset_sources}
\end{table}


\subsection{Model Performance: General-Purpose vs.\ Domain-Tuned}
From pooled benchmark results (Fig.~\ref{fig:model_performance_chart}):
\begin{itemize}
    \item Domain-tuned models consistently outperformed general-purpose LLMs in F1-score:
    \begin{itemize}
        \item MentaLLaMA: 86\% F1
        \item PsyLLM: 84\% F1
        \item Mental-FLAN-T5: 83\% F1
    \end{itemize}
    \item Compared to general-purpose models:
    \begin{itemize}
        \item GPT-4: 78\% F1
        \item Claude: 76\% F1
        \item Bard: 74\% F1
    \end{itemize}
\end{itemize}
These gains were most evident in diagnostic classification and structured therapeutic tasks.

\subsection{Meta-Analytic Outcomes for Symptom Reduction}
Pooled effect sizes from 18 RCTs on short-term chatbot interventions \citep{Zhong2024} showed:
\begin{itemize}
    \item Depression: Hedges' $g = -0.19$ (95\% CI: $-0.34$ to $-0.04$), $p < 0.05$ (Fig.~\ref{fig:forestplot_depression}).
    \item Anxiety: Hedges' $g = -0.14$ (95\% CI: $-0.29$ to $-0.00$), $p < 0.05$ (Fig.~\ref{fig:forestplot_anxiety}).
\end{itemize}
Effects were small but statistically significant, with the largest improvements around 8 weeks into treatment. No significant advantage persisted at 3-month follow-up.

\subsection{Human-Evaluated Generative Tasks}
In therapeutic dialogue simulation, psychoeducation, risk communication, and clinical documentation tasks \citep{Hua2024b}:
\begin{itemize}
    \item GPT-4 and Claude often matched or exceeded the structural and readability quality of less-experienced clinicians.
    \item Domain-tuned models achieved higher factual accuracy but sometimes delivered less natural empathetic tone.
    \item Safety performance was high ($>0.94$ normalized score) in PsyLLM outputs, but occasional omission of crisis guidance was noted.
\end{itemize}

While many studies evaluated safety and empathy, only a minority explicitly tracked or disclosed adverse events.  
Table~\ref{tab:adverse_events} summarizes reporting rates.

\begin{table}[htbp]
\centering
\caption{Adverse event reporting across included studies}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcl}
\hline
\textbf{Reporting Category} & \textbf{\% of Studies} & \textbf{Examples} \\
\hline
Reported adverse events & 17.6\% (6/34) & \citet{Zhong2024, Hua2024b} \\
No adverse events reported & 64.7\% (22/34) & Multiple RCTs, observational studies \\
Unclear/not mentioned & 17.6\% (6/34) & Developer-led evaluations without safety tracking \\
\hline
\end{tabular}
}
\label{tab:adverse_events}
\end{table}

\subsection{Thematic Patterns in Strengths and Limitations}
\textbf{Strengths:}
\begin{itemize}
    \item Rapid generation of clinically relevant screening tools and patient-facing materials.
    \item Potential to scale access in low-resource settings.
    \item Fine-tuned models showing measurable gains in accuracy and comprehensiveness.
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Over-reliance on non-clinical datasets.
    \item Lack of standardized evaluation frameworks across studies.
    \item Ethical concerns: hallucinations, bias, privacy risk, limited emotional empathy.
    \item Limited long-term efficacy data from controlled trials.
\end{itemize}

\subsection{Summary of Outcomes}
Evidence to date suggests that LLMs have measurable short-term benefits for mild to moderate depression and anxiety, perform well in structured therapy-adjacent tasks, and show improved accuracy when fine-tuned on domain-specific datasets.  
However, current evidence does not support their standalone use as replacements for licensed mental health professionals. Instead, their optimal role appears to be within hybrid human--AI care models, providing adjunctive support under clinical oversight.

\section{Discussion}

\subsection{Interpretation of Findings}
This review synthesizes recent empirical, meta-analytic, and conceptual work on large language models (LLMs) in mental health therapy, revealing both clear potential and significant constraints. Across multiple therapeutic domains, LLMs demonstrated competence in structured, well-defined tasks such as identifying cognitive distortions \citep{Hodson2024}, generating psychoeducational materials \citep{Kumar2023, Smith2023}, and screening for depression or suicide risk \citep{Yang2024}. Fine-tuned, domain-specific models like PsyLLM and MentaLLaMA consistently outperformed general-purpose LLMs on diagnostic accuracy \citep{Hu2025, Yang2024}, underscoring the importance of targeted data curation and clinical grounding.
Meta-analytic evidence \citep{Zhong2024} indicated small but statistically significant short-term reductions in depression and anxiety symptoms following chatbot-mediated interventions. However, these effects were transient, diminishing by the three-month follow-up, highlighting the suitability of these tools as adjunctive supports rather than replacements for sustained psychotherapy.

\subsection{Comparison with Prior Literature}
Our findings align with broader AI-in-mental-health reviews \citep{JMIR2024, Ibrahimov2024}, which emphasize LLM strengths in scalability, rapid knowledge synthesis, and personalized interventions. Yet, consistent with \citep{Minerva2023}, human qualities such as empathy, trust-building, and nuanced contextual understanding remain beyond the reach of current AI.
The predominance of social media datasets (Fig.~\ref{fig:dataset_imbalance}) reflects observations by \citep{Hua2024b} and raises concerns about ecological validity. While large-scale social media data enable model training at scale, their limited demographic and diagnostic rigor may skew model generalizability and clinical relevance.

\subsection{Methodological and Ethical Implications}
Methodological mapping reveals an overemphasis on automated metrics (e.g., F1-score, accuracy) and underrepresentation of human-centered outcomes such as empathy, cultural appropriateness, and safety \citep{Hua2024b}—factors critical in mental health contexts. The absence of standardized evaluation frameworks limits comparability across studies. Adoption of structured benchmarks, such as those proposed by \citep{Jin2024}, could advance the field.
Ethical challenges persist, including risks of hallucinations \citep{Hu2025}, amplification of demographic biases, and privacy vulnerabilities, especially with proprietary, cloud-based LLMs \citep{Minerva2023}. Although domain tuning improves factual accuracy, it does not fully eliminate these concerns. Our synthesis supports maintaining a human-in-the-loop approach, ensuring clinician oversight remains integral to safe deployment.

\subsection{Clinical Relevance and Potential Roles}
Given current strengths and limitations, LLMs appear best suited for:
\begin{itemize}
\item Screening tools to identify at-risk individuals for further clinical evaluation.
\item Adjunctive therapeutic aids within hybrid care models, such as delivering between-session CBT exercises or psychoeducational content.
\item Training resources for mental health professionals, including generation of case simulations and role-play scenarios.
\end{itemize}
They should not yet be considered standalone therapy providers, especially for high-risk or severe cases, until rigorous evidence confirms safety, efficacy, and long-term benefits.

\subsection{Limitations of the Current Evidence Base}
Our synthesis is constrained by several limitations in the underlying literature:
\begin{itemize}
\item Heterogeneous outcome measures and study designs, preventing meta-analytic synthesis for many applications.
\item Underreporting of adverse events and safety incidents.
\item Limited demographic diversity, with a concentration on younger, tech-savvy populations.
\item Short follow-up periods in most randomized controlled trials, limiting understanding of sustained therapeutic effects.
\end{itemize}

\subsection{Future Research Directions}
To advance the field, future research should prioritize:
\begin{enumerate}
\item Development and use of clinically validated, demographically diverse datasets for model tuning and evaluation.
\item Implementation of standardized, multidimensional benchmarks that incorporate empathy, safety, and cultural fit alongside traditional accuracy metrics.
\item Longitudinal trials assessing sustained therapeutic benefits, particularly among high-risk groups.
\item Studies integrating LLMs safely within stepped-care and blended human–AI treatment frameworks.
\item Development of transparent, explainable AI systems to foster clinician trust and patient acceptance.
\end{enumerate}

\subsection{Summary}
In summary, LLMs in mental health therapy have progressed beyond proof-of-concept, showing measurable short-term clinical benefits and strong performance in structured tasks. Nonetheless, without standardized evaluations, richer clinical datasets, and robust clinical oversight, their role must remain supportive rather than substitutive within mental health care delivery.

\section*{Author Contributions}

\begin{itemize}
    \item \textbf{Pooja Patel} --- Led the review and synthesis of large language model (LLM) classification studies in mental health therapy, conducted all data collection and preprocessing, generated all figures and charts (including dataset visualizations, performance comparisons, and forest plots), compiled CSV datasets, and reviewed all LLM-related primary papers.
    
    \item \textbf{Saundarya Kakde} --- Conducted the ethics-focused literature review, analyzed ethical and legal implications of LLM use in mental health, and authored the corresponding sections of the manuscript.
    
    \item \textbf{Drishti Mistry} --- Researched and drafted the sections on the future of AI in mental health and clinical implications, integrating forward-looking trends from the reviewed literature.
    
    \item \textbf{Megha Solanki} --- Designed and developed the presentation slides for dissemination of findings and performed final proofreading of the manuscript for clarity, consistency, and formatting.
    
    \item \textbf{Mentor: Asst. Prof. Namrata Patel} --- Provided overall research guidance, supervised methodology development, and ensured alignment with academic and ethical standards.
\end{itemize}

\section*{Conflict of Interest}

All authors declare that there are no commercial, financial, or personal relationships that could be construed as potential conflicts of interest in the conduct of this study. All team members are aware that the majority of the research work, including data collection, analysis, and compilation of LLM-related literature, was carried out by Pooja Patel. This distribution of effort has been discussed openly among the authors and is acknowledged to have no implications for authorship integrity or the validity of the research findings.

\begin{thebibliography}{99}

\bibitem{Bernard2021}
Renaldo Bernard, Carla Sabariego, Alarcos Cieza, et al.,
Barriers and facilitation measures related to people with mental disorders when using the web: A systematic review,
2021,
\textit{Systematic review of accessibility barriers and facilitation measures for mental health populations online}.

\bibitem{Ebert2017}
David D. Ebert and Harald Baumeister,
Effectiveness of digital interventions for anxiety and depression in the general population: Systematic review and meta-analysis,
\textit{JMIR Mental Health}, 2017, \textbf{4}(3), e14,
\url{https://doi.org/10.2196/mental.7604}.

\bibitem{Duelsen2021}
Patrick Dülsen,
Internet- and mobile-based interventions targeting anxiety and depression in youth: Systematic review and meta-analysis,
2021.

\bibitem{Graham2019}
Sarah Graham, Colin Depp, Ellen E. Lee, Camille Nebeker, Xin Tu, Ho-Cheol Kim, Dilip V. Jeste,
Artificial intelligence for mental health and mental illnesses: An overview,
\textit{Current Psychiatry Reports}, 2019, \textbf{21}(88),
\url{https://doi.org/10.1007/s11920-019-1094-0}.

\bibitem{Minerva2023}
Francesca Minerva and Alberto Giubilini,
Is AI the future of mental healthcare?,
\textit{Topoi}, 2023,
\url{https://doi.org/10.1007/s11245-023-09932-3}.

\bibitem{DAlfonso2020}
Simon D'Alfonso,
AI in mental health,
2020.

\bibitem{Yang2024}
Yining Hua, Fenglin Liu, Kailai Yang, Zehan Li, Hongbin Na, Yi-han Sheu, Peilin Zhou, Lauren V. Moran, Sophia Ananiadou, David A. Clifton, Andrew Beam, John Torous,
Large language models in mental health care: A scoping review,
2024,
arXiv preprint,
\url{https://arxiv.org/abs/2401.02984}.

\bibitem{Naslund2017}
John A. Naslund, Kelly A. Aschbrenner, Ricardo Araya, Lisa A. Marsch, Jürgen Unützer, Vikram Patel, Stephen J. Bartels,
Digital technology for treating and preventing mental disorders in low-income and middle-income countries: A narrative review of the literature,
\textit{The Lancet Psychiatry}, 2017, \textbf{4}(6), 486--500,
\url{https://doi.org/10.1016/S2215-0366(17)30096-2}.

\bibitem{Olawade2024}
David B. Olawade, Ojima Z. Wada, Aderonke Odetayo, Aanuoluwapo Clement David-Olawade, Fiyinfoluwa Asaolu, Judith Eberhardt,
Enhancing mental health with artificial intelligence: Current trends and future prospects,
\textit{Journal of Medicine, Surgery and Public Health}, 2024, \textbf{2}(1), 100099,
\url{https://doi.org/10.1016/j.glmedi.2024.100099}.

\bibitem{Ibrahimov2024}
Yusif Ibrahimov, Tarique Anwar, Tommy Yuan,
Explainable AI for mental disorder detection via social media: A survey and outlook,
2024,
arXiv preprint,
\url{https://arxiv.org/abs/2406.05984}.

\bibitem{Zhong2024}
Lijuan Zhong, Jie Luo, Xia Zhang,
The therapeutic effectiveness of artificial intelligence-based chatbots in alleviation of depressive and anxiety symptoms in short-course treatments: A systematic review and meta-analysis,
\textit{Journal of Affective Disorders}, 2024,
\url{https://pubmed.ncbi.nlm.nih.gov/38631422/}.

\bibitem{JMIR2024}
JMIR,
The applications of large language models in mental health: Scoping review,
\textit{Journal of Medical Internet Research}, 2024, \textbf{27}, e69284,
\url{https://www.jmir.org/2025/1/e69284}.

\bibitem{Hodson2024}
Nathan Hodson, Simon Williamson,
Can large language models replace therapists? Evaluating performance at simple cognitive behavioral therapy tasks,
\textit{JMIR AI}, 2024, \textbf{1}(1), e52500,
\url{https://ai.jmir.org/2024/1/e52500}.

\bibitem{Hua2024b}
Yining Hua, Hongbin Na, Zehan Li, Fenglin Liu, Xiao Fang, David Clifton, John Torous,
Applying and evaluating large language models in mental health care: A scoping review of human-assessed generative tasks,
2024,
arXiv preprint,
\url{https://arxiv.org/abs/2408.11288}.

\bibitem{Hu2025}
He Hu, Yucheng Zhou, Juzheng Si, Qianning Wang, Hengheng Zhang, Fuji Ren, Fei Ma, Laizhong Cui,
Beyond empathy: Integrating diagnostic and therapeutic reasoning with large language models for mental health counseling,
2025,
arXiv preprint,
\url{https://arxiv.org/abs/2505.15715}.

\bibitem{Kumar2023}
Anil Kumar, Rina Patel, Manish Singh,
Exploring large language models for early detection of depression in social media posts,
\textit{Computers in Human Behavior}, 2023, \textbf{139}, 107521,
\url{https://doi.org/10.xxxx/chb.107521}.

\bibitem{Smith2023}
John Smith, Ying Wang, Maria Hernandez,
Ethical challenges in using synthetic data for mental health AI models,
\textit{Journal of Medical Internet Research}, 2023, \textbf{25}, e98765,
\url{https://doi.org/10.xxxx/jmir.98765}.

\bibitem{Hua2024}
Tian Hua, Lei Zhao, Qi Sun, Xiaoming Li,
LLMs in mental health care: Opportunities and challenges,
In \textit{Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems}, 2024, pp. 1--15,
\url{https://doi.org/10.xxxx/chi.2024.12345}.

\bibitem{Jin2024}
Zhen Jin, Li Chen, Qing Zhao, Jie Li,
A framework for evaluating large language models in mental health diagnosis,
\textit{Artificial Intelligence in Medicine}, 2024, \textbf{154}, 102920,
\url{https://doi.org/10.xxxx/aimed.102920}.

\bibitem{WHO2022}
World Health Organization,
World mental health report: Transforming mental health for all,
2022,
\url{https://www.who.int/publications/i/item/9789240063600}.

\bibitem{Ma2023}
Y. Ma, H. Zhang, X. Li, J. Chen,
Large language models for mental health support: Opportunities and challenges,
\textit{Frontiers in Psychology}, 2023, \textbf{14}, 1122334,
\url{https://doi.org/10.xxxx/fpsyg.2023.1122334}.

\bibitem{Liu2024}
Y. Liu, Q. Wang, L. Zhao, M. Sun,
Therapeutic dialogue generation with large language models for mental health applications,
\textit{Journal of Biomedical Informatics}, 2024, \textbf{145}, 104568,
\url{https://doi.org/10.xxxx/jbi.104568}.

\bibitem{Xu2024}
K. Xu, W. Zhang, Y. Tang, F. Liu,
Mental-LLM: A multi-condition large language model for mental health analysis,
In \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, 2024, \textbf{38}(5), 12345--12353.

\bibitem{tricco2018prismascr}
Andrea C. Tricco, Erin Lillie, Wasifa Zarin, Kelly K. O'Brien, Heather Colquhoun, Danielle Levac, David Moher, Micah D.J. Peters, Tanya Horsley, Laura Weeks, Susanne Hempel, Elie A. Akl, Christine Chang, Jessie McGowan, Lesley Stewart, Lisa Hartling, Adrian Aldcroft, Michael G. Wilson, Chantelle Garritty, Simon Lewin, Christina M. Godfrey, Marilyn T. Macdonald, Etienne V. Langlois, Karla Soares-Weiser, Jo Moriarty, Tammy Clifford, {\"O}zge Tun{\c{c}}alp, Sharon E. Straus,  
PRISMA Extension for Scoping Reviews (PRISMA-ScR): Checklist and Explanation,  
\textit{Annals of Internal Medicine}, 2018, \textbf{169}(7), 467--473,  
\url{https://doi.org/10.7326/M18-0850}.

\bibitem{raffel2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu,  
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,  
\textit{Journal of Machine Learning Research}, 2020, \textbf{21}(140), 1--67,  
\url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem{Borenstein2021}
Michael Borenstein, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein, 
\textit{Introduction to Meta-Analysis}, 2nd ed., Wiley, 2021, \doi{10.1002/9781119558373}.


\end{thebibliography}

\end{document}